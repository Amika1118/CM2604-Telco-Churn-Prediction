# -*- coding: utf-8 -*-
"""CM2604-Telco-Churn-Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ML-CXsHr78tdzjlr8J4AG5r0WCqq9Pna
"""

#creating the directories
import os
import warnings
warnings.filterwarnings("ignore")

base_dir = "CM2604-Telco-Churn-Prediction"

folders = [
    f"{base_dir}/data",
    f"{base_dir}/explainability",
    f"{base_dir}/models",
    f"{base_dir}/plots",
    f"{base_dir}/preprocessing",
    f"{base_dir}/reports"
]

for f in folders:
    os.makedirs(f, exist_ok=True)

print("Folder structure created:")
for f in folders:
    print(" -",f)

# PART 2 - Install packages and import libraries
!pip install scikit-learn==1.3.0 pandas==2.0.3 numpy==1.24.3 matplotlib==3.7.2 seaborn==0.12.2 tensorflow==2.19.0 shap==0.46.0 joblib==1.3.2 -q

import io
import joblib
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, roc_curve
)
from sklearn.utils.class_weight import compute_class_weight

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

import shap

sns.set(style="whitegrid")

print("Packages and libraries installed")

# PART 3 - Load dataset and preprocessing
from google.colab import files

print("Upload dataset file: WA_Fn-UseC_-Telco-Customer-Churn.csv")
uploaded = files.upload()
filename = list(uploaded.keys())[0]

df = pd.read_csv(io.BytesIO(uploaded[filename]))
df.to_csv(f"{base_dir}/data/dataset.csv", index=False)

# Clean TotalCharges and encode target
df["TotalCharges"] = pd.to_numeric(df["TotalCharges"], errors="coerce")
df["TotalCharges"].fillna(df["TotalCharges"].median(), inplace=True)
df["Churn"] = df["Churn"].map({"No": 0, "Yes": 1})

# Drop customerID if present
if "customerID" in df.columns:
    df = df.drop(columns=["customerID"])

# Split features and target
X = df.drop(columns=["Churn"])
y = df["Churn"]

# Identify column types
categorical_cols = X.select_dtypes(include=["object"]).columns.tolist()
numerical_cols = X.select_dtypes(include=["int64", "float64"]).columns.tolist()

# Preprocessing pipeline
numerical_transformer = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

categorical_transformer = Pipeline([
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=False))
])

preprocessor = ColumnTransformer([
    ("num", numerical_transformer, numerical_cols),
    ("cat", categorical_transformer, categorical_cols)
])

# Save preprocessor
joblib.dump(preprocessor, f"{base_dir}/preprocessing/preprocessor.pkl")

# Train-test split and preprocessing transform
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20, stratify=y, random_state=42
)

X_train_p = preprocessor.fit_transform(X_train)
X_test_p = preprocessor.transform(X_test)

# Class weights for imbalance
class_weights = compute_class_weight("balanced", classes=np.unique(y_train), y=y_train)
cw = {0: float(class_weights[0]), 1: float(class_weights[1])}

print("Preprocessing complete.")
print("Training samples:", X_train_p.shape[0], "Test samples:", X_test_p.shape[0])

# PART 4 - EDA: basic plots and save results
# Churn distribution
plt.figure(figsize=(5,4))
sns.countplot(x=df["Churn"].map({0:"No",1:"Yes"}))
plt.title("Churn Distribution")
plt.xlabel("Churn")
plt.ylabel("Count")
plt.tight_layout()
plt.savefig(f"{base_dir}/plots/churn_distribution.png", dpi=300)
plt.close()

# Numerical histograms
num_cols = numerical_cols.copy()
plt.figure(figsize=(12, 8))
df[num_cols].hist(bins=30, layout=(len(num_cols)//3+1,3), figsize=(12,8))
plt.tight_layout()
plt.savefig(f"{base_dir}/plots/numerical_histograms.png", dpi=300)
plt.close()

# Correlation heatmap for numeric features including Churn
corr_df = df[num_cols + ["Churn"]]
plt.figure(figsize=(8,6))
sns.heatmap(corr_df.corr(), annot=True, fmt=".2f", cmap="coolwarm")
plt.title("Correlation Matrix")
plt.tight_layout()
plt.savefig(f"{base_dir}/plots/correlation_matrix.png", dpi=300)
plt.close()

# Save short EDA insights
insights = []
insights.append(f"Dataset shape: {df.shape}")
insights.append(f"Overall churn rate: {df['Churn'].mean():.3f}")
for col in ["Contract","InternetService","PaymentMethod","SeniorCitizen"]:
    if col in df.columns:
        rates = df.groupby(col)["Churn"].mean().sort_values(ascending=False)*100
        top = rates.index[0]
        insights.append(f"Highest churn group in {col}: {top} ({rates.iloc[0]:.1f}%)")

with open(f"{base_dir}/reports/eda_insights.txt", "w") as f:
    f.write("\n".join(insights))

print("EDA complete. Plots saved to:", f"{base_dir}/plots")
print("EDA insights saved to:", f"{base_dir}/reports/eda_insights.txt")

# PART 5 — Evaluation utility functions
from sklearn.metrics import classification_report

def evaluate_model_print_save(name, y_true, y_pred, y_proba, save_path_txt, save_path_cm=None):
    acc = accuracy_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred, zero_division=0)
    rec = recall_score(y_true, y_pred, zero_division=0)
    f1 = f1_score(y_true, y_pred, zero_division=0)
    auc = roc_auc_score(y_true, y_proba)
    cm = confusion_matrix(y_true, y_pred)

    # Print results
    print("==================================================")
    print(f"MODEL RESULTS: {name}")
    print("==================================================")
    print(f"Accuracy  : {acc:.4f}")
    print(f"Precision : {prec:.4f}")
    print(f"Recall    : {rec:.4f}")
    print(f"F1-Score  : {f1:.4f}")
    print(f"ROC-AUC   : {auc:.4f}")
    print("Confusion Matrix:")
    print(cm)
    print("==================================================\n")

    # Save summary to text file
    with open(save_path_txt, "w") as f:
        f.write(f"Model: {name}\n")
        f.write(f"Accuracy: {acc:.4f}\n")
        f.write(f"Precision: {prec:.4f}\n")
        f.write(f"Recall: {rec:.4f}\n")
        f.write(f"F1-Score: {f1:.4f}\n")
        f.write(f"ROC-AUC: {auc:.4f}\n")
        f.write("Confusion Matrix:\n")
        f.write(str(cm) + "\n")

    # Save confusion matrix as plot
    if save_path_cm is not None:
        plt.figure(figsize=(4,3))
        sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
        plt.title(f"{name} - Confusion Matrix")
        plt.xlabel("Predicted")
        plt.ylabel("Actual")
        plt.tight_layout()
        plt.savefig(save_path_cm, dpi=300)
        plt.close()

# PART 6 — Decision Tree Baseline
dt_baseline = DecisionTreeClassifier(max_depth=5, criterion="gini", random_state=42)
dt_baseline.fit(X_train_p, y_train)

y_pred_dt_base = dt_baseline.predict(X_test_p)
y_proba_dt_base = dt_baseline.predict_proba(X_test_p)[:,1]

evaluate_model_print_save(
    "Decision Tree (Baseline)",
    y_test, y_pred_dt_base, y_proba_dt_base,
    save_path_txt=f"{base_dir}/reports/dt_baseline.txt",
    save_path_cm=f"{base_dir}/plots/dt_baseline_cm.png"
)

# Save ROC plot
fpr, tpr, _ = roc_curve(y_test, y_proba_dt_base)
plt.figure()
plt.plot(fpr, tpr, label=f"AUC={roc_auc_score(y_test, y_proba_dt_base):.3f}")
plt.plot([0,1],[0,1],"--", color="gray")
plt.title("Decision Tree Baseline ROC")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()
plt.tight_layout()
plt.savefig(f"{base_dir}/plots/dt_baseline_roc.png", dpi=300)
plt.close()

joblib.dump(dt_baseline, f"{base_dir}/models/dt_baseline.pkl")
print("Saved Decision Tree baseline model to:", f"{base_dir}/models/dt_baseline.pkl")

# PART 7 — Decision Tree Manual Tuning
manual_params = []
max_depths = [4, 6, 8, 10]
criteria = ["gini", "entropy"]
min_splits = [2, 5]
min_leafs = [1, 2]

for md in max_depths:
    for crit in criteria:
        for ms in min_splits:
            for ml in min_leafs:
                manual_params.append({"max_depth": md, "criterion": crit, "min_samples_split": ms, "min_samples_leaf": ml})

best_auc = -1
best_model = None
best_params = None
tuning_results = []

for params in manual_params:
    model = DecisionTreeClassifier(
        max_depth=params["max_depth"],
        criterion=params["criterion"],
        min_samples_split=params["min_samples_split"],
        min_samples_leaf=params["min_samples_leaf"],
        random_state=42
    )
    model.fit(X_train_p, y_train)
    y_prob = model.predict_proba(X_test_p)[:,1]
    auc = roc_auc_score(y_test, y_prob)

    # Save result record
    tuning_results.append({
        "params": params,
        "auc": float(auc)
    })

    # Print brief progress (one line per model)
    print(f"Tested params: {params} -> AUC: {auc:.4f}")

    if auc > best_auc:
        best_auc = auc
        best_model = model
        best_params = params

# Evaluate best manual model
y_pred_dt_tuned = best_model.predict(X_test_p)
y_proba_dt_tuned = best_model.predict_proba(X_test_p)[:,1]

evaluate_model_print_save(
    "Decision Tree (Manually Tuned)",
    y_test, y_pred_dt_tuned, y_proba_dt_tuned,
    save_path_txt=f"{base_dir}/reports/dt_tuned.txt",
    save_path_cm=f"{base_dir}/plots/dt_tuned_cm.png"
)

# Save ROC plot for best
fpr, tpr, _ = roc_curve(y_test, y_proba_dt_tuned)
plt.figure()
plt.plot(fpr, tpr, label=f"AUC={best_auc:.3f}")
plt.plot([0,1],[0,1],"--", color="gray")
plt.title("Decision Tree (Manually Tuned) ROC")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()
plt.tight_layout()
plt.savefig(f"{base_dir}/plots/dt_tuned_roc.png", dpi=300)
plt.close()

joblib.dump(best_model, f"{base_dir}/models/dt_tuned.pkl")

# Save tuning summary CSV
import csv
tuning_csv = f"{base_dir}/reports/dt_manual_tuning_results.csv"
with open(tuning_csv, "w", newline="") as csvfile:
    fieldnames = ["max_depth","criterion","min_samples_split","min_samples_leaf","auc"]
    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
    writer.writeheader()
    for r in tuning_results:
        p = r["params"]
        writer.writerow({
            "max_depth": p["max_depth"],
            "criterion": p["criterion"],
            "min_samples_split": p["min_samples_split"],
            "min_samples_leaf": p["min_samples_leaf"],
            "auc": r["auc"]
        })

print("Best manual DT params:", best_params, "AUC:", best_auc)
print("Manual tuning results saved to:", tuning_csv)
print("Saved tuned DT model to:", f"{base_dir}/models/dt_tuned.pkl")

# PART 8 — Neural Network Baseline
nn_baseline = keras.Sequential([
    layers.Dense(32, activation="relu", input_shape=(X_train_p.shape[1],)),
    layers.Dense(16, activation="relu"),
    layers.Dense(1, activation="sigmoid")
])

nn_baseline.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])

history_baseline = nn_baseline.fit(
    X_train_p, y_train,
    validation_split=0.2,
    epochs=20,
    batch_size=32,
    verbose=1
)

# Predictions and evaluation
y_proba_nn_base = nn_baseline.predict(X_test_p).flatten()
y_pred_nn_base = (y_proba_nn_base > 0.5).astype(int)

evaluate_model_print_save(
    "Neural Network (Baseline)",
    y_test, y_pred_nn_base, y_proba_nn_base,
    save_path_txt=f"{base_dir}/reports/nn_baseline.txt",
    save_path_cm=f"{base_dir}/plots/nn_baseline_cm.png"
)

# Save ROC and history plots
fpr, tpr, _ = roc_curve(y_test, y_proba_nn_base)
plt.figure()
plt.plot(fpr, tpr, label=f"AUC={roc_auc_score(y_test, y_proba_nn_base):.3f}")
plt.plot([0,1],[0,1],"--", color="gray")
plt.title("NN Baseline ROC")
plt.xlabel("FPR")
plt.ylabel("TPR")
plt.legend()
plt.tight_layout()
plt.savefig(f"{base_dir}/plots/nn_baseline_roc.png", dpi=300)
plt.close()

# Training history plots
plt.figure()
plt.plot(history_baseline.history["loss"], label="train_loss")
plt.plot(history_baseline.history["val_loss"], label="val_loss")
plt.title("NN Baseline Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.tight_layout()
plt.savefig(f"{base_dir}/plots/nn_baseline_loss.png", dpi=300)
plt.close()

plt.figure()
plt.plot(history_baseline.history["accuracy"], label="train_acc")
plt.plot(history_baseline.history["val_accuracy"], label="val_acc")
plt.title("NN Baseline Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.tight_layout()
plt.savefig(f"{base_dir}/plots/nn_baseline_acc.png", dpi=300)
plt.close()

nn_baseline.save(f"{base_dir}/models/nn_baseline.h5")
print("Saved NN baseline model to:", f"{base_dir}/models/nn_baseline.h5")

# PART 9 — Neural Network Manually Tuned
nn_tuned = keras.Sequential([
    layers.Dense(256, activation="relu", input_shape=(X_train_p.shape[1],)),
    layers.Dropout(0.3),
    layers.Dense(128, activation="relu"),
    layers.Dropout(0.3),
    layers.Dense(64, activation="relu"),
    layers.Dense(1, activation="sigmoid")
])

nn_tuned.compile(
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    loss="binary_crossentropy",
    metrics=["accuracy"]
)

callbacks_list = [
    keras.callbacks.EarlyStopping(monitor="val_loss", patience=8, restore_best_weights=True),
    keras.callbacks.ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=4, min_lr=1e-5)
]

history_tuned = nn_tuned.fit(
    X_train_p, y_train,
    validation_split=0.2,
    epochs=50,
    batch_size=32,
    callbacks=callbacks_list,
    verbose=1
)

# Predictions and evaluation
y_proba_nn_tuned = nn_tuned.predict(X_test_p).flatten()
y_pred_nn_tuned = (y_proba_nn_tuned > 0.5).astype(int)

evaluate_model_print_save(
    "Neural Network (Manually Tuned)",
    y_test, y_pred_nn_tuned, y_proba_nn_tuned,
    save_path_txt=f"{base_dir}/reports/nn_tuned.txt",
    save_path_cm=f"{base_dir}/plots/nn_tuned_cm.png"
)

# Save ROC plot and history plots
fpr, tpr, _ = roc_curve(y_test, y_proba_nn_tuned)
plt.figure()
plt.plot(fpr, tpr, label=f"AUC={roc_auc_score(y_test, y_proba_nn_tuned):.3f}")
plt.plot([0,1],[0,1],"--", color="gray")
plt.title("NN Tuned ROC")
plt.xlabel("FPR")
plt.ylabel("TPR")
plt.legend()
plt.tight_layout()
plt.savefig(f"{base_dir}/plots/nn_tuned_roc.png", dpi=300)
plt.close()

# Loss and accuracy history
plt.figure()
plt.plot(history_tuned.history["loss"], label="train_loss")
plt.plot(history_tuned.history["val_loss"], label="val_loss")
plt.title("NN Tuned Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.tight_layout()
plt.savefig(f"{base_dir}/plots/nn_tuned_loss.png", dpi=300)
plt.close()

plt.figure()
plt.plot(history_tuned.history["accuracy"], label="train_acc")
plt.plot(history_tuned.history["val_accuracy"], label="val_acc")
plt.title("NN Tuned Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.tight_layout()
plt.savefig(f"{base_dir}/plots/nn_tuned_acc.png", dpi=300)
plt.close()

nn_tuned.save(f"{base_dir}/models/nn_tuned.h5")
print("Saved NN tuned model to:", f"{base_dir}/models/nn_tuned.h5")

# PART 10 — Build comparison table from saved reports
import pandas as pd

def load_metrics_from_txt(path):
    metrics = {}
    with open(path, "r") as f:
        for line in f:
            if ":" in line:
                k, v = line.split(":", 1)
                k = k.strip()
                v = v.strip()
                try:
                    metrics[k] = float(v)
                except:
                    metrics[k] = v
    return metrics

paths = {
    "DT Baseline": f"{base_dir}/reports/dt_baseline.txt",
    "DT Tuned": f"{base_dir}/reports/dt_tuned.txt",
    "NN Baseline": f"{base_dir}/reports/nn_baseline.txt",
    "NN Tuned": f"{base_dir}/reports/nn_tuned.txt"
}

rows = []
for name, p in paths.items():
    m = load_metrics_from_txt(p)
    rows.append({
        "Model": name,
        "Accuracy": m.get("Accuracy", np.nan),
        "Precision": m.get("Precision", np.nan),
        "Recall": m.get("Recall", np.nan),
        "F1-Score": m.get("F1-Score", np.nan),
        "AUC": m.get("ROC-AUC", m.get("ROC-AUC", m.get("ROC-AUC:", np.nan))) if isinstance(m.get("ROC-AUC", np.nan), float) else m.get("ROC-AUC", m.get("AUC", np.nan))
    })

comparison_df = pd.DataFrame(rows).set_index("Model")
comparison_df.to_csv(f"{base_dir}/reports/model_comparison.csv")
with open(f"{base_dir}/reports/model_comparison.txt", "w") as f:
    f.write("MODEL COMPARISON\n")
    f.write("================\n\n")
    f.write(comparison_df.to_string())

print("Model comparison saved to:")
print(" - CSV:", f"{base_dir}/reports/model_comparison.csv")
print(" - TXT:", f"{base_dir}/reports/model_comparison.txt")

# Print comparison table
display(comparison_df)

# PART 11 — Ethics, bias and deployment notes
ethical_framework = [
    "Fairness: monitor and mitigate bias across protected groups (gender, age).",
    "Transparency: provide explanations for high-risk predictions (feature importances, SHAP).",
    "Privacy: remove PII, secure stored models, follow GDPR where applicable.",
    "Accountability: maintain logs and human-in-the-loop for retention actions."
]

with open(f"{base_dir}/reports/ethical_framework.txt", "w") as f:
    f.write("\n".join(ethical_framework))

deployment_strategy = [
    "Monitoring: track accuracy, recall, AUC, and data drift; alert on degradation.",
    "Retraining: schedule periodic retraining and validation with new data.",
    "Explainability: produce per-prediction explanations for business users.",
    "Infrastructure: containerized deployment, CI/CD, model and data versioning."
]

with open(f"{base_dir}/reports/deployment_strategy.txt", "w") as f:
    f.write("\n".join(deployment_strategy))

bias_analysis_text = [
    "Bias analysis: evaluate model performance on subgroups (e.g., SeniorCitizen, gender).",
    "Recommended mitigation: reweighting, monitoring, fairness-aware thresholds, human review."
]

with open(f"{base_dir}/reports/bias_analysis.txt", "w") as f:
    f.write("\n".join(bias_analysis_text))

roadmap = [
    "Weeks 1-2: Final validation, automation of preprocessing, tests.",
    "Weeks 3-4: Staging evaluation and A/B tests.",
    "Weeks 5-8: Production rollout and CRM integration.",
    "Weeks 9-12: Monitoring and iterative improvements."
]

with open(f"{base_dir}/reports/implementation_roadmap.txt", "w") as f:
    f.write("\n".join(roadmap))

print("Task 3 files saved to:", f"{base_dir}/reports")

# PART 12 — SHAP explainability for tuned NN (optional)
explainer = shap.Explainer(nn_tuned.predict, X_train_p)   # model.predict used directly
shap_values = explainer(X_test_p[:200])

plt.figure()
shap.plots.beeswarm(shap_values, max_display=15, show=False)
plt.savefig(f"{base_dir}/explainability/shap_beeswarm.png", dpi=300, bbox_inches="tight")
plt.close()
print("SHAP beeswarm saved to:", f"{base_dir}/explainability/shap_beeswarm.png")

import os

# Assuming base_dir is defined from previous cells
if 'base_dir' in globals() and os.path.exists(base_dir):
    print(f"The project directory '{base_dir}' exists.")
else:
    print(f"The project directory '{base_dir}' does NOT exist. Please ensure PART 1 (creating directories) was executed successfully.")

#@markdown ### ⚠️ Please enter your GitHub details:
import os

USERNAME = "" #@param {type:"string"}
REPOSITORY_NAME = "" #@param {type:"string"}



# Select 'repo' scope.
TOKEN = "" #@param {type:"string"}

# Construct the GitHub URL with PAT for authentication
GITHUB_URL = f"https://{USERNAME}:{TOKEN}@github.com/{USERNAME}/{REPOSITORY_NAME}.git"

if os.getcwd() != "/content":
    os.chdir("/content")
    print("Changed current directory to /content.")

if not os.path.exists(base_dir):
    print(f"Error: Directory '{base_dir}' not found. Please ensure PART 1 (creating directories) was executed successfully.")
else:
    os.chdir(base_dir)
    print(f"Changed current directory to: {os.getcwd()}")


print(f"Attempting to push files in {os.getcwd()} to GitHub repository: {USERNAME}/{REPOSITORY_NAME}")

# Initialize Git repository if not already initialized
if not os.path.exists(".git"):
    !git init
    print("Initialized empty Git repository.")
else:
    print("Git repository already initialized.")

# Set Git user details (optional, but good practice)
!git config user.name "{USERNAME}"
!git config user.email "{USERNAME}@users.noreply.github.com" # Or your actual email

# Add all files in the current directory to the staging area
!git add .
print("Added all files to staging.")

# Commit the changes
!git commit -m "Initial commit from Colab: Project files and results" --allow-empty
print("Committed changes.")

result = !git remote -v
if not any("origin" in s for s in result):
    !git remote add origin {GITHUB_URL}
    print("Added remote 'origin'.")
else:
    print("Remote 'origin' already exists. Updating URL if necessary.")
    !git remote set-url origin {GITHUB_URL}

# Determine the current branch name
current_branch = !git rev-parse --abbrev-ref HEAD
current_branch = current_branch[0].strip()
print(f"Current Git branch detected: {current_branch}")

# Push the changes to GitHub
try:
    !git push -u origin {current_branch}
    print(f"Successfully pushed to GitHub on branch '{current_branch}'!")
except Exception as e:
    print(f"Failed to push to branch '{current_branch}': {e}")
    print("Please check your GitHub URL, token, and repository settings.")